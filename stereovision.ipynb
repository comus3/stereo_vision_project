{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereovision\n",
    "\n",
    "![Suzanne](main.png)\n",
    "\n",
    "Stereovision is a discipline that deals with the reconstruction of 3D information from images. For the reconstruction of a point, several images of this point are needed. These images must be taken from different points of view. The key step of the reconstruction, which is often problematic, is to identify the images of the point to be reconstructed in each view.\n",
    "\n",
    "## Epipolar Geometry\n",
    "\n",
    "Epipolar geometry involves two cameras. The epipolar geometry describes the geometric properties between two views of the same scene and depends only on the intrinsic parameters of the cameras and their relative positions. It provides, in particular, the epipolar constraint, which will be very useful to produce the matches between views.\n",
    "\n",
    "## The Fondamental Matrix\n",
    "\n",
    "![Epipolar Geometry - Sanyam Kapoor](epipolar.png)\n",
    "\n",
    "Let us imagine that we have two images, right and left, of the world space. Let's take a point $\\vec{x}$ in the right image space. The point $\\vec{X}$ of the world space, of which $\\vec{x}$ is the image, can be anywhere on the line passing through $\\vec{x}$ and the optical center of the right camera. We will call this line the back-projected ray of $\\vec{x}$. Let us note $\\vec{x}'$ the image of $\\vec{X}$ in the left image space. The locus of $\\vec{x}'$ is therefore the image line of the back-projected ray of $\\vec{x}$. This line is called the epipolar line and is denoted $\\vec{l}'$. The epipolar line passes through the epipole $\\vec{e}'$, image of the optical center of the right camera.\n",
    "\n",
    "In 2D projective geometry, a line with equation $ax+by+c = 0$ is represented by a vector with three components $(a, b, c)^T$ defined to within one factor. Thus, we have the following relationship:\n",
    "\n",
    ">The point $\\vec{x}$ belongs to the line $\\vec{l}$ if and only if $x^T\\vec{l} = 0$.\n",
    "\n",
    "Moreover, in 2D projective geometry, the following remarkable relations are valid:\n",
    "\n",
    "- The intersection of two lines $l$ and $l'$ is given by $x = l \\times l'$,\n",
    "- The line passing through two points $x$ and $x'$ is given by $l = x \\times x'$.\n",
    "\n",
    "Note that the vector product can be written as a product of matrix $x \\times y = [x]_\\times y$ where\n",
    "\n",
    "$$[x]_\\times = \\begin{pmatrix} 0 & −x3 & x2 \\\\ x3 & 0 & −x1 \\\\ −x2 & x1 & 0 \\end{pmatrix}$$\n",
    "\n",
    "To find the equation of the epipolar line in the left image space, we just need to find the coordinates of two points of this line. The first is the image $P'\\vec{C}$ of the optical center $\\vec{C}$ of the right camera where $P'$ is the projection matrix of the left camera. The second is $P'P^{+}\\vec{x}$ where $P^{+}$ is the pseudo inverse of the projection matrix $P$ of the right camera. The epipolar line thus has the equation $l' = [P'\\vec{C}]_\\times{}P'P^{+}\\vec{x} = F\\vec{x}$ with $F = [P'\\vec{C}]_\\times{}P'P^{+}$. $F$ is called fundamental matrix.\n",
    "\n",
    "Since the epipolar line $\\vec{l}' = F\\vec{x}$ is the locus of $\\vec{x}'$, $\\vec{x}'$ therefore belongs to $\\vec{l}'$ which leads to the epipolar constraint :\n",
    "\n",
    ">**The fundamental matrix is such that for any pair of points corresponding $\\vec{x} \\leftrightarrow \\vec{x}'$ in the two images, we have $\\vec{x}'^{T}F\\vec{x} = 0$.**\n",
    "\n",
    "## Computation of the fundamental matrix\n",
    "\n",
    "The fundamental matrix $F$ has seven degrees of freedom. It has nine components but these are defined to within one scale factor, which removes one degree of freedom. Moreover, the matrix $F$ is a singular matrix ($det(F) = 0$) which gives us seven degrees of freedom. So we need at least seven correspondences to compute $F$. The equation $x'^{T}_iFx_i = 0$ and the seven correspondences allow us to write a system of equations of the form $Af = 0$, where $f$ is the vector which contains the components of the matrix $F$. Let us assume that $A$ is a 7×9 matrix of rank 7. The general solution of $Af = 0$ can be written $\\alpha f_1 + (1-\\alpha) f_2$ where $f_1$ and $f_2$ are two particular independent solutions of $Af = 0$. We then use the singularity constraint $det(\\alpha F_1 + (1 - \\alpha)F_2) = 0$ to determine $\\alpha$. Since the singularity constraint gives rise to a third degree equation, we may have one or three solutions for $F$.\n",
    "\n",
    "## OpenCV\n",
    "\n",
    "In practice you will use the OpenCV library. In python, you have access to its functions through the `cv2` module.\n",
    "\n",
    "You can find help with the calibration and reconstruction functions on the site https://docs.opencv.org/4.0.0/d9/d0c/group__calib3d.html\n",
    "\n",
    "## Goal\n",
    "\n",
    "In the zip of the statement you will find two sequences of images taken by two cameras during the scanning of an object by a laser plane.\n",
    "\n",
    "![Laser](scanRight/scan0010.png)\n",
    "\n",
    "You will also find shots of a checkerboard in different positions that will help you calibrate your cameras.\n",
    "\n",
    "![Damier](chessboards/c2Right.png)\n",
    "\n",
    "The goal is to reconstruct the scanned object in 3D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the following is the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import cv2 as cv\n",
    "import glob\n",
    "from mathutils import geometry as pygeo\n",
    "from mathutils import Vector\n",
    "import json\n",
    "from utils import *\n",
    "import cv2 as cv\n",
    "import glob\n",
    "import regex as re\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage, display\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable initialisation\n",
    "\n",
    "\n",
    "This section will calibrate the cameras constants for later stereovision    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define stopping criteria for corner refinement\n",
    "criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n",
    "\n",
    "# Prepare 3D points of the checkerboard assuming it's flat (z=0)\n",
    "objp = np.zeros((7 * 7, 3), np.float32)\n",
    "objp[:, :2] = np.mgrid[0:7, 0:7].T.reshape(-1, 2)\n",
    "\n",
    "# Lists to store 3D real-world points and 2D image points for all images\n",
    "objpoints = []\n",
    "imgpoints = []\n",
    "\n",
    "# Load checkerboard images to calibrate the camera\n",
    "images = glob.glob('chessboards/c4*.png')\n",
    "\n",
    "# Loop over each image\n",
    "for fname in images:\n",
    "    img = cv.imread(fname)\n",
    "    gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect checkerboard corners\n",
    "    ret, corners = cv.findChessboardCorners(gray, (7, 7), None)\n",
    "\n",
    "    if ret:  # If corners are successfully detected\n",
    "        objpoints.append(objp)\n",
    "\n",
    "        # Refine corner positions to sub-pixel accuracy\n",
    "        corners = cv.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "        # Draw corners on the image\n",
    "        img_with_corners = img.copy()\n",
    "        cv.drawChessboardCorners(img_with_corners, (7, 7), corners, ret)\n",
    "\n",
    "        # Convert the image to RGB for Matplotlib\n",
    "        img_rgb = cv.cvtColor(img_with_corners, cv.COLOR_BGR2RGB)\n",
    "\n",
    "        # Plot using Matplotlib\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img_rgb)\n",
    "        plt.title(f\"Detected Corners in {fname}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "# Calibrate the camera using the collected 3D and 2D points\n",
    "ret, mtx, dist, rvecs, tvecs = cv.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "\n",
    "# Print calibration results\n",
    "print(\"Camera Calibration Results:\")\n",
    "print(f\"Reprojection error: {ret}\")\n",
    "print(f\"Camera matrix (mtx):\\n{mtx}\")\n",
    "print(f\"Distortion coefficients (dist):\\n{dist}\")\n",
    "print(f\"Rotation vectors (rvecs):\\n{rvecs}\")\n",
    "print(f\"Translation vectors (tvecs):\\n{tvecs}\")\n",
    "\n",
    "\n",
    "# Convert rotation vectors to rotation matrices\n",
    "rot_mats = [cv.Rodrigues(rvec)[0] for rvec in rvecs]\n",
    "# Why: Rotation vectors are converted to rotation matrices because they are easier to use in further computations \n",
    "# like creating projection matrices or performing triangulation.\n",
    "\n",
    "# Compute projection matrices for each image\n",
    "proj_mats = []\n",
    "for i in range(len(rot_mats)):\n",
    "    rot_trans_mat = np.hstack((rot_mats[i], tvecs[i]))  # Combine rotation and translation into a single matrix\n",
    "    proj_mat = mtx @ rot_trans_mat  # Compute the projection matrix\n",
    "    proj_mats.append(proj_mat)\n",
    "# Why: Projection matrices encode how 3D points in the real world map to 2D image points, combining intrinsic \n",
    "# and extrinsic camera parameters.\n",
    "\n",
    "# Compute the fundamental matrix between two images\n",
    "# For simplicity, select two images to calculate the fundamental matrix\n",
    "img1, img2 = images[:2]\n",
    "gray1 = cv.cvtColor(cv.imread(img1), cv.COLOR_BGR2GRAY)\n",
    "gray2 = cv.cvtColor(cv.imread(img2), cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect ORB keypoints and descriptors\n",
    "orb = cv.ORB_create()  # Create ORB detector\n",
    "kp1, des1 = orb.detectAndCompute(gray1, None)\n",
    "kp2, des2 = orb.detectAndCompute(gray2, None)\n",
    "# Why: ORB is a fast and robust feature detector and descriptor that works well for finding matching points.\n",
    "\n",
    "# Match keypoints using BFMatcher\n",
    "bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1, des2)\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "# Why: Feature matching identifies corresponding points between two images, which are needed to compute the \n",
    "# fundamental matrix and triangulate 3D points.\n",
    "\n",
    "# Extract matched points\n",
    "pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "# Why: Extracting the coordinates of matched points allows further geometric computations.\n",
    "\n",
    "# Compute the fundamental matrix\n",
    "F, mask = cv.findFundamentalMat(pts1, pts2, cv.FM_RANSAC)\n",
    "print(\"Fundamental Matrix (F):\\n\", F)\n",
    "# Why: The fundamental matrix encodes the geometric relationship between two images, \n",
    "# which is essential for tasks like epipolar geometry and 3D reconstruction.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert rotation vectors to rotation matrices\n",
    "rmatRight = cv.Rodrigues(rvecs[0])[0]  # Rotation matrix for the right camera\n",
    "rmatLeft = cv.Rodrigues(rvecs[1])[0]  # Rotation matrix for the left camera\n",
    "# Why: The Rodrigues transformation converts rotation vectors into 3x3 rotation matrices, \n",
    "# which are easier to work with when constructing projection matrices.\n",
    "\n",
    "# Construct projection matrices by combining rotation and translation\n",
    "rotMatRight = np.concatenate((rmatRight, tvecs[0]), axis=1)  # 3x4 matrix for the right camera\n",
    "rotMatLeft = np.concatenate((rmatLeft, tvecs[1]), axis=1)  # 3x4 matrix for the left camera\n",
    "# Why: The projection matrices encode how points in the 3D world are mapped to the 2D image planes of each camera.\n",
    "\n",
    "# Compute the projection matrices of the cameras\n",
    "camLeft = mtx @ rotMatLeft  # Projection matrix for the left camera\n",
    "camRight = mtx @ rotMatRight  # Projection matrix for the right camera\n",
    "# Why: These projection matrices combine intrinsic and extrinsic parameters, allowing us to project any 3D point \n",
    "# in the world onto the corresponding 2D image plane.\n",
    "\n",
    "# Compute the world coordinates of the camera centers (homogeneous coordinates)\n",
    "camWorldCenterLeft = np.linalg.inv(\n",
    "    np.concatenate((rotMatLeft, [[0, 0, 0, 1]]), axis=0)\n",
    ") @ np.transpose([[0, 0, 0, 1]])\n",
    "camWorldCenterRight = np.linalg.inv(\n",
    "    np.concatenate((rotMatRight, [[0, 0, 0, 1]]), axis=0)\n",
    ") @ np.transpose([[0, 0, 0, 1]])\n",
    "# Why: Calculating the camera centers in the world coordinate system is essential for understanding the geometry \n",
    "# of the scene and verifying the relative positions of the cameras.\n",
    "\n",
    "print(\"Matrice de projection de la caméra gauche :\")\n",
    "print(camLeft)\n",
    "print(\"\\nMatrice de projection de la caméra droite :\")\n",
    "print(camRight)\n",
    "# Why: Printing the projection matrices helps verify their correctness. These matrices are fundamental for computing \n",
    "# the epipolar geometry and reconstructing 3D points from image pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot les cameras dans l'espace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDotWorld(camWorldCenterLeft,camWorldCenterRight,objp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour stocker les frames du GIF\n",
    "frames = []\n",
    "\n",
    "# Parcourir toutes les images du dossier spécifié (par exemple, \"scanRight\")\n",
    "image_files = glob.glob(\"scanRight/*.png\")  # Remplacez par votre chemin si nécessaire\n",
    "\n",
    "for img_file in sorted(image_files):  # Trier pour garder un ordre cohérent\n",
    "    # Charger l'image\n",
    "    img = cv.imread(img_file)  # Charge chaque image dans une variable\n",
    "    \n",
    "    # Appliquer un seuil pour extraire la ligne\n",
    "    ret, mask = cv.threshold(img, 127, 255, cv.THRESH_TOZERO)\n",
    "    \n",
    "    # Convertir le masque en format compatible avec PIL (RVB)\n",
    "    mask_rgb = cv.cvtColor(mask, cv.COLOR_BGR2RGB)\n",
    "    pil_image = Image.fromarray(mask_rgb)  # Convertir l'image en objet PIL\n",
    "    \n",
    "    # Ajouter l'image traitée à la liste des frames\n",
    "    frames.append(pil_image)\n",
    "\n",
    "# Générer le GIF\n",
    "output_gif_path = \"output.gif\"  # Nom du fichier GIF généré\n",
    "frames[0].save(\n",
    "    output_gif_path,\n",
    "    save_all=True,\n",
    "    append_images=frames[1:],  # Ajouter toutes les frames sauf la première\n",
    "    duration=200,  # Durée de chaque frame en millisecondes\n",
    "    loop=0  # 0 = boucle infinie\n",
    ")\n",
    "\n",
    "# Afficher le GIF dans Jupyter\n",
    "display(IPImage(output_gif_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Fundamental Matrix and Epipolar Geometry\n",
    "\n",
    "## 1. Calculating the Fundamental Matrix\n",
    "\n",
    "In this section, we compute the **Fundamental Matrix** (`Fondamental`), which defines the relationship between corresponding points in the left and right camera views.\n",
    "\n",
    "### Code Explanation:\n",
    "```python\n",
    "Fondamental = matFondamental(camRight, camWorldCenterLeft, camLeft)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Fundamental Matrix\n",
    "Fondamental = matFondamental(camRight, camWorldCenterLeft, camLeft)\n",
    "# This creates the fundamental matrix, which relates points in the left camera's view \n",
    "# to corresponding epipolar lines in the right camera's view.\n",
    "# Inputs:\n",
    "# - camRight: The projection matrix of the right camera (3x4).\n",
    "# - camWorldCenterLeft: The position of the left camera in world coordinates (4x1 homogeneous).\n",
    "# - camLeft: The projection matrix of the left camera (3x4).\n",
    "# Process:\n",
    "# The function combines the intrinsic and extrinsic camera parameters, along with\n",
    "# the pseudo-inverse of the right camera matrix, to calculate a 3x3 fundamental matrix.\n",
    "# Output:\n",
    "# - Fondamental: Encodes the epipolar geometry between the two cameras.\n",
    "\n",
    "# Compute the epipolar geometry for images in the 'scanLeft/' folder\n",
    "epl = findEpilines('scanLeft/', Fondamental)\n",
    "# This calculates the epilines for each image in the sequence located in 'scanLeft/'.\n",
    "# Inputs:\n",
    "# - 'scanLeft/': The folder containing the input images, which are assumed to show a laser line.\n",
    "# - Fondamental: The fundamental matrix, used to compute epilines for each point.\n",
    "# Process:\n",
    "# - For each image in the folder:\n",
    "#   1. `getImgLine` isolates the red laser line in the image (removes other details).\n",
    "#   2. It calculates the average X position of red pixels for each row (laser line points).\n",
    "#   3. These points (`pointsLeft`) are transformed using the fundamental matrix\n",
    "#      to compute the corresponding epipolar lines (`epilinesRight`) in the right image.\n",
    "# - The epipolar lines and points are stored for later use.\n",
    "# Output:\n",
    "# - `epl`: A list of `[pointsLeft, epilinesRight]` for all images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing the EPLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "process_folder(\n",
    "    left_folder='scanLeft',\n",
    "    right_folder='scanRight',\n",
    "    EplLeft=epl,  \n",
    "    EplRight=epl, \n",
    "    output_gif_path='output_1.gif',\n",
    "    duration=300\n",
    ")\n",
    "\n",
    "display(pythImage(filename='output_1.gif'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsRight = eplRedPoints('scanRight/scan',epl)\n",
    "pointsLeft = eplRedPoints('scanLeft/',epl)\n",
    "\n",
    "point = getObjectPoint(pointsRight,epl,camWorldCenterLeft,camWorldCenterRight,camLeft,camRight)\n",
    "drawSurfaceObject(point)\n",
    "drawPointObject(point)\n",
    "pointToJson(point)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
